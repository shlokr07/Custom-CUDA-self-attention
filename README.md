# Custom-CUDA-self-attention
Implementing self-attention from scratch and comparing it with PyTorch and Tensorflow's attention layers
